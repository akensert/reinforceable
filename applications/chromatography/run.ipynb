{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eecceb-3a9a-4597-9109-0dd0a0f99ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import reinforceable\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "from networks import EncoderNetwork\n",
    "from networks import PolicyNetwork\n",
    "from networks import ValueNetwork\n",
    "\n",
    "from env import Chromatography \n",
    "\n",
    "# Try to make runs reproducible:\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Helper function to save agent during run:\n",
    "\n",
    "def save_agent(agent, path, batch_shape=[None, None]):\n",
    "    agent.save(\n",
    "        path, \n",
    "        reinforceable.Timestep(\n",
    "            state={\n",
    "                'chromatogram': tf.TensorSpec(shape=batch_shape + [8192, 1], dtype=tf.float32), \n",
    "                'phi_target': tf.TensorSpec(shape=batch_shape + [NUM_ACTIONS], dtype=tf.float32)\n",
    "            }, \n",
    "            step_type=tf.TensorSpec(shape=batch_shape + [1], dtype=tf.int32), \n",
    "            reward=tf.TensorSpec(shape=batch_shape + [1], dtype=tf.float32),\n",
    "            info={}\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac427976-8717-4947-9acd-180f107389bf",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b384c7c0-24ee-4609-8651-8fe399b5ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop settings\n",
    "ITERS = 3_500     # main loop\n",
    "STEPS = 1024      # interaction loop\n",
    "BATCH_SIZE = 32   # training loop\n",
    "REPEATS = 4       # training loop\n",
    "\n",
    "# Environment settings\n",
    "MAX_TIME = 20.0\n",
    "PARALLEL_ENVS = 8\n",
    "NUM_COMPOUNDS = (10, 20)\n",
    "NUM_EXPERIMENTS = 1               # Episode length\n",
    "NUM_SEGMENTS = 3              \n",
    "NUM_ACTIONS = NUM_SEGMENTS + 1    # + 1 for initial phi\n",
    "CHROMATOGRAM_SHAPE = (8192, 1)\n",
    "PHI_SHAPE = (NUM_ACTIONS,)\n",
    "    \n",
    "# Agent settings and PPO hyperparameters\n",
    "LR_INITIAL = 1e-4\n",
    "LR_END = 1e-6\n",
    "LR_DECAY_STEPS = 50_000\n",
    "\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "LAMBDA_FACTOR = 0.95\n",
    "USE_GAE = True\n",
    "USE_TD_LAMBDA_RETURN = False \n",
    "VALUE_LOSS_COEF = 0.5\n",
    "POLICY_LOSS_COEF = 1.0\n",
    "ENTROPY_LOSS_COEF = 0.01\n",
    "KL_CUTOFF_FACTOR = 2.0\n",
    "KL_CUTOFF_COEF = 1000.0\n",
    "KL_BETA_INITIAL = 1.0\n",
    "KL_TARGET = 0.01\n",
    "KL_TOLERANCE = 0.3\n",
    "GRADIENT_CLIPPING = 0.5\n",
    "VALUE_CLIPPING = 0.2\n",
    "IMPORTANCE_RATIO_CLIPPING = 0.2\n",
    "REWARD_NORMALIZATION = False\n",
    "STATE_NORMALIZATION = False\n",
    "ADVANTAGE_NORMALIZATION = True\n",
    "\n",
    "AGENT_NAME = f'Agent-S{NUM_ACTIONS-1:02d}E{NUM_EXPERIMENTS:02d}'\n",
    "SUMMARY_WRITER = f'./logs/' + AGENT_NAME\n",
    "AGENT_PATH = f'./saved_agents/' + AGENT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9592245d-f1fc-475e-a48c-c341bf7da15a",
   "metadata": {},
   "source": [
    "## Build Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d029a542-4bb6-4bd6-9258-036908885d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(num_actions, num_experiments):\n",
    "    async_env = reinforceable.envs.AsyncEnvironment(\n",
    "        [\n",
    "            lambda i=i: Chromatography(\n",
    "                time=MAX_TIME,\n",
    "                num_actions=NUM_ACTIONS,\n",
    "                num_experiments=NUM_EXPERIMENTS,\n",
    "                num_compounds=NUM_COMPOUNDS,\n",
    "                seed=i * 1_000_000\n",
    "            ) for i in range(PARALLEL_ENVS)\n",
    "        ],\n",
    "    )\n",
    "    return async_env\n",
    "\n",
    "async_env = get_env(NUM_ACTIONS, NUM_EXPERIMENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ea2378-8caa-4a96-bfa5-0e8da0ef4d84",
   "metadata": {},
   "source": [
    "## Build Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b488ac6-7485-46ea-b44d-d35235d0985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent():\n",
    "    \n",
    "    encoder_network = EncoderNetwork(PARALLEL_ENVS, CHROMATOGRAM_SHAPE, PHI_SHAPE)\n",
    "    policy_network = PolicyNetwork(encoder_network.output, (NUM_ACTIONS,))\n",
    "    value_network = ValueNetwork(encoder_network.output)\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(\n",
    "       keras.optimizers.schedules.PolynomialDecay(\n",
    "            initial_learning_rate=LR_INITIAL,\n",
    "            decay_steps=LR_DECAY_STEPS,\n",
    "            end_learning_rate=LR_END,\n",
    "            power=2.0,\n",
    "       )\n",
    "    )\n",
    "    \n",
    "    agent = reinforceable.agents.RecurrentPPOAgent(\n",
    "        encoder_network,\n",
    "        policy_network,\n",
    "        value_network,\n",
    "        optimizer=optimizer,\n",
    "        discount_factor=DISCOUNT_FACTOR,\n",
    "        lambda_factor=LAMBDA_FACTOR,\n",
    "        use_gae=USE_GAE,\n",
    "        use_td_lambda_return=USE_TD_LAMBDA_RETURN,\n",
    "        value_loss_coef=VALUE_LOSS_COEF,\n",
    "        policy_loss_coef=POLICY_LOSS_COEF,\n",
    "        entropy_loss_coef=ENTROPY_LOSS_COEF,\n",
    "        kl_cutoff_factor=KL_CUTOFF_FACTOR,\n",
    "        kl_cutoff_coef=KL_CUTOFF_COEF,\n",
    "        kl_beta_initial=KL_BETA_INITIAL,\n",
    "        kl_target=KL_TARGET,\n",
    "        kl_tolerance=KL_TOLERANCE,\n",
    "        gradient_clip=GRADIENT_CLIPPING,\n",
    "        value_clip=VALUE_CLIPPING,\n",
    "        importance_ratio_clip=IMPORTANCE_RATIO_CLIPPING,\n",
    "        reward_normalization=REWARD_NORMALIZATION,\n",
    "        state_normalization=STATE_NORMALIZATION,\n",
    "        advantage_normalization=ADVANTAGE_NORMALIZATION,\n",
    "        summary_writer=SUMMARY_WRITER,\n",
    "    )\n",
    "\n",
    "    return agent\n",
    "\n",
    "agent = get_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0c2d8a-9097-4f7b-94bb-7a514cbe233d",
   "metadata": {},
   "source": [
    "## Run\n",
    "\n",
    "> Observe training progression, by launching tensorboard (from command line): `tensorboard --logdir ./logs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7470c9f9-b914-4efb-b5e6-0cfbdda15858",
   "metadata": {},
   "outputs": [],
   "source": [
    "observers = {\n",
    "    'episode_return': reinforceable.utils.observers.RollingAverageEpisodeReturn(128), \n",
    "    'steps': reinforceable.utils.observers.StepCounter(),\n",
    "    'episodes': reinforceable.utils.observers.EpisodeCounter(),\n",
    "}\n",
    "\n",
    "driver = reinforceable.Driver(agent, async_env, observers)\n",
    "\n",
    "best_return = float('-inf')\n",
    "\n",
    "for i in (pbar := tqdm(range(ITERS))):\n",
    "\n",
    "    data = driver.run(steps=STEPS)\n",
    "\n",
    "    loss = agent.train(data, batch_size=BATCH_SIZE, repeats=REPEATS)\n",
    "\n",
    "    result = driver.result()\n",
    "    \n",
    "    pbar.set_description(\n",
    "        f'average return: {result[\"episode_return\"]:.2f}\\t-\\t'\n",
    "        f'total steps: {int(result[\"steps\"]):,}\\t-\\t'\n",
    "        f'total episodes: {int(result[\"episodes\"]):,}\\t-\\t'\n",
    "    )\n",
    "\n",
    "    # Use agent's tf summary writer to log rolling average episode returns.\n",
    "    if agent.summary_writer is not None:\n",
    "        with agent.summary_writer.as_default():\n",
    "            tf.summary.scalar('episode_return', result['episode_return'], result['steps'])\n",
    "\n",
    "    # If agent is peak performing, save it\n",
    "    if i > (ITERS/400) and result['episode_return'] > best_return:\n",
    "        best_return = result['episode_return']\n",
    "        save_agent(agent, AGENT_PATH)\n",
    "        \n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdea1a9-8540-417f-8997-2ec9315b0a00",
   "metadata": {},
   "source": [
    "## Load agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f234c7-d7d0-4c40-abb8-11a65d324684",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_agent = tf.saved_model.load(AGENT_PATH)\n",
    "\n",
    "# Obtain initial timestep (containing initial state (separation)):\n",
    "timestep = async_env.reset()\n",
    "# Need to add time dimension as agent is recurrent:\n",
    "timestep = tf.nest.map_structure(lambda x: tf.expand_dims(x, 0), timestep)\n",
    "# Compute action:\n",
    "action, _ = loaded_agent(timestep)\n",
    "# Perform action:\n",
    "action = tf.nest.map_structure(lambda x: tf.squeeze(x, 0), action)\n",
    "async_env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ec0c05-2911-4976-aaa3-86c612aee47b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
