{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b93379-f66b-4415-972b-4b0454c29c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import reinforceable\n",
    "from reinforceable import agents \n",
    "from reinforceable import envs \n",
    "from reinforceable import layers\n",
    "from reinforceable import utils\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b52fef-0e4a-41fa-af03-e18bf6296efe",
   "metadata": {},
   "source": [
    "## 1. Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25378ec-9856-4345-9e30-ad53f22477a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 'BeamRiderNoFrameskip-v4'\n",
    "full_action_space = False\n",
    "seed = 1\n",
    "\n",
    "def make_env(seed, render_mode=None):\n",
    "\n",
    "    env = gym.make(\n",
    "        id, \n",
    "        render_mode=render_mode, \n",
    "        full_action_space=full_action_space)\n",
    "\n",
    "    # Adding more prepocessing steps (env wrappers) will likely \n",
    "    # improve learning of the agent significantly.\n",
    "    # For instance, scaling of the reward between -1 and 1. \n",
    "    env = gym.wrappers.AtariPreprocessing(\n",
    "        env, \n",
    "        noop_max=0, \n",
    "        frame_skip=8, \n",
    "        screen_size=84, \n",
    "        terminal_on_life_loss=False, \n",
    "        grayscale_obs=True, \n",
    "        grayscale_newaxis=True, \n",
    "        scale_obs=False\n",
    "    )\n",
    "\n",
    "    env = envs.gym_wrappers.FloatingStateEnv(env)\n",
    "    env = envs.gym_wrappers.EpisodicLifeEnv(env)\n",
    "    env = envs.gym_wrappers.NoInfoEnv(env)\n",
    "    env = envs.gym_wrappers.TimestepEnv(env)\n",
    "\n",
    "    env.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    \n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e671e88e-6d5a-412d-9c83-c192d6983002",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_shape = (32,)\n",
    "state_shape = make_env(seed).observation_space.shape\n",
    "num_actions = make_env(seed).action_space.n\n",
    "\n",
    "env_constructors = [\n",
    "    lambda: make_env(seed + i) for i in range(batch_shape[0])\n",
    "]\n",
    "\n",
    "output_signature = reinforceable.Timestep(\n",
    "    state=tf.TensorSpec(batch_shape + state_shape, dtype=tf.float32), \n",
    "    reward=tf.TensorSpec(batch_shape + (1,), dtype=tf.float32),\n",
    "    step_type=tf.TensorSpec(batch_shape + (1,), dtype=tf.int32),                                    \n",
    "    info={},\n",
    ")\n",
    "\n",
    "env = envs.AsyncEnvironment(\n",
    "    env_constructors,\n",
    "    output_signature=output_signature)\n",
    "\n",
    "# Visualize initial state of the first environment\n",
    "plt.imshow(env.reset().state[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bf0b56-c823-4f52-84fe-938ba994d548",
   "metadata": {},
   "source": [
    "## 2. Build networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0b2b91-1127-40e8-8623-717bc0349ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(batch_shape + state_shape, dtype=tf.float32)\n",
    "states_mask = keras.layers.Input(batch_shape + (1,), dtype=tf.bool)\n",
    "\n",
    "# Note: TimeDistributed wrapper only really needed for Flatten() below, though good to be explicit.\n",
    "\n",
    "# Build encoder network\n",
    "x = inputs\n",
    "x = keras.layers.TimeDistributed(keras.layers.Conv2D(32, 8, strides=4, activation='relu'))(x)\n",
    "x = keras.layers.TimeDistributed(keras.layers.Conv2D(64, 4, strides=2, activation='relu'))(x)\n",
    "x = keras.layers.TimeDistributed(keras.layers.Conv2D(64, 3, strides=1, activation='relu'))(x)\n",
    "x = keras.layers.TimeDistributed(keras.layers.Flatten())(x)\n",
    "x = keras.layers.TimeDistributed(keras.layers.Dense(512, activation='relu'))(x)\n",
    "encodings = layers.StatefulRNN(keras.layers.LSTMCell(128))(x, states_mask)\n",
    "\n",
    "encoder_network = keras.Model((inputs, states_mask), encodings, name='encoder_network') \n",
    "\n",
    "# Build policy network (shares encoder network)\n",
    "x = keras.layers.TimeDistributed(keras.layers.Dense(512, activation='relu'))(encoder_network.output)\n",
    "distribs = layers.DenseCategorical((num_actions,))(x)\n",
    "policy_network = keras.Model(encoder_network.output, distribs, name='policy_network')\n",
    "\n",
    "# Build value network (shares encoder network)\n",
    "x = keras.layers.TimeDistributed(keras.layers.Dense(512, activation='relu'))(encoder_network.output)\n",
    "values = keras.layers.TimeDistributed(keras.layers.Dense(1))(x)\n",
    "value_network = keras.Model(encoder_network.output, values, name='value_network')\n",
    "\n",
    "# Visualize networks\n",
    "print(encoder_network.summary())\n",
    "print(policy_network.summary())\n",
    "print(value_network.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1844e0-2c6c-4af5-8f7f-b9332a612393",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 3. Create agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035ad47c-a059-4d71-ac2d-ededcaf71944",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(\n",
    "   keras.optimizers.schedules.PolynomialDecay(\n",
    "        initial_learning_rate=3e-4,\n",
    "        decay_steps=500_000,\n",
    "        end_learning_rate=1e-5,\n",
    "   )\n",
    ")\n",
    "\n",
    "agent = agents.RecurrentPPOAgent(\n",
    "    encoder_network,\n",
    "    policy_network,\n",
    "    value_network,\n",
    "    optimizer=optimizer,\n",
    "    discount_factor=0.995,\n",
    "    lambda_factor=0.95,\n",
    "    use_gae=True,\n",
    "    use_td_lambda_return=False,\n",
    "    value_loss_coef=0.2,\n",
    "    policy_loss_coef=1.0,\n",
    "    entropy_loss_coef=0.01,\n",
    "    kl_cutoff_factor=2.0,\n",
    "    kl_cutoff_coef=1000.0,\n",
    "    kl_beta_initial=1.0,\n",
    "    kl_target=0.01,\n",
    "    kl_tolerance=0.3,\n",
    "    gradient_clip=0.5,\n",
    "    value_clip=0.1,\n",
    "    importance_ratio_clip=0.1,\n",
    "    reward_normalization=True,\n",
    "    state_normalization=True,\n",
    "    advantage_normalization=True,\n",
    "    summary_writer='/tmp/mylogs/recurrent_ppo_agent',\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62e2bf3-bab6-415d-bc3f-efb0df189171",
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 10000   # total number of iterations (10k calls to driver.run and agent.train)\n",
    "steps = 4096    # total steps (128 for each of the 32 environments)\n",
    "\n",
    "observers = {\n",
    "    'episode length': utils.observers.RollingAverageEpisodeLength(10), \n",
    "    'episode return': utils.observers.RollingAverageEpisodeReturn(10), \n",
    "    'steps': utils.observers.StepCounter(),\n",
    "    'episodes': utils.observers.EpisodeCounter(),\n",
    "}\n",
    "\n",
    "driver = reinforceable.Driver(agent, env, observers)\n",
    "\n",
    "for i in (pbar := tqdm(range(iters))):\n",
    "\n",
    "    data = driver.run(steps=steps)\n",
    "\n",
    "    loss = agent.train(data, batch_size=32, repeats=4)\n",
    "\n",
    "    result = driver.result()\n",
    "    \n",
    "    pbar.set_description(\n",
    "        f'average return: {result[\"episode return\"]:.2f}\\t-\\t'\n",
    "        f'average length: {int(result[\"episode length\"]):,}\\t-\\t'\n",
    "        f'total steps: {int(result[\"steps\"]):,}\\t-\\t'\n",
    "        f'total episodes: {int(result[\"episodes\"]):,}\\t-\\t'\n",
    "    )\n",
    "\n",
    "    # Use agent summary writer to add rolling average episode return and length\n",
    "    # To write to summaries (to to tensorboard), pass a path to summary_writer. \n",
    "    # Then from terminal, run `tensorboard --logdir path`\n",
    "    if agent.summary_writer is not None:\n",
    "        with agent.summary_writer.as_default():\n",
    "            tf.summary.scalar('episode_return', result['episode return'], result['steps'])\n",
    "            tf.summary.scalar('episode_length', result['episode length'], result['steps'])\n",
    "        \n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3423b22c-0b93-424b-88e2-2f7322168c75",
   "metadata": {},
   "source": [
    "## 4. Debug "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a7e1e3-67ed-4fb7-be93-456725109dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_env = envs.AsyncEnvironment(\n",
    "    [lambda: make_env(seed, render_mode='human')],\n",
    "    output_signature=reinforceable.Timestep(\n",
    "        state=tf.TensorSpec((1,) + state_shape, dtype=tf.float32), \n",
    "        reward=tf.TensorSpec((1,) + (1,), dtype=tf.float32),\n",
    "        step_type=tf.TensorSpec((1,) + (1,), dtype=tf.int32),\n",
    "        info={}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0e022a-a35e-4d09-a43f-de14e112ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_return, episode_length = agent._play(play_env, deterministic=False, pad=batch_shape[0]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f174018-2e0b-4f2c-be20-670e6f229772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
